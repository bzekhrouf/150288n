name: Mise à jour automatique de la liste des backups (nouvelle page)

on:
  schedule:
    - cron: '0 6 * * *'  # Tous les jours à 6h UTC
  workflow_dispatch:     # Lancement manuel

jobs:
  update-backups:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Télécharger la page et extraire les liens avec curl + grep/sed
        run: |
          # Télécharge la page avec un User-Agent navigateur (contourne souvent le 403)
          curl -s -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36" \
            "https://www.novaler.com/downloads/enigma2-backups2/multibox-4k-pro-ultra-hd" -o page.html

          # Extraire toutes les lignes contenant un lien .zip (les URLs directes sont dans href)
          # Puis nettoyer pour avoir "Description https://url.zip"
          grep -oP '(?<=href=")[^"]*\.zip' page.html | \
            sed 's|^|https://www.novaler.com|' | \
            paste - <(grep -Poz '(?s)<p[^>]*>.*?(<strong>|<b>)?\K[^<]*(?=</strong>|</b>|</p>)' page.html | tr '\n' '\f' | sed 's/\f/ /g; s/  */ /g') > backups.txt || echo "Erreur lors de l'extraction"

          # Version simplifiée plus robuste : on extrait juste les URLs directes et on crée des descriptions basées sur le nom de fichier
          echo "Novaler4K_UHD_MultiboxPro" > backups.txt
          grep -o 'https://www.novaler.com/uploads/enigmapro/[^"]*\.zip' page.html | sort -r >> backups.txt

          rm page.html

      - name: Commit et push si modifié
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add backups.txt
          git diff --quiet && echo "Aucune modification" || (git commit -m "Mise à jour automatique des backups (nouvelle page)" && git push)
